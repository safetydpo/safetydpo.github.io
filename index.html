<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation">
  <meta name="keywords" content="text-to-image, safety, DPO, alignment, RLHF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Runtao Liu<sup>1</sup><span style="font-size: smaller;">*</span>,</span>
            <span class="author-block">I Chieh Chen<sup>1</sup><span style="font-size: smaller;">*</span>,</span>
            <span class="author-block">Jindong Gu<sup>2</sup>,</span>
            <span class="author-block">Jipeng Zhang<sup>1</sup>,</span>
            <span class="author-block">Renjie Pi<sup>1</sup>,</span>
            <br>
            <span class="author-block">Qifeng Chen<sup>1</sup>,</span>
            <span class="author-block">Philip Torr<sup>2</sup>,</span>
            <span class="author-block">Ashkan Khakzar<sup>2</sup>,</span>
            <span class="author-block">Fabio Pizzati<sup>2</sup></span>
        </div>
        <div class="is-size-5 publication-authors">
          <span class="author-block"><sup>1</sup>Hong Kong University of Science and Technology,</span>
          <span class="author-block"><sup>2</sup>University of Oxford</span>
        </div>
        <div class="is-size-6" style="margin-top: 10px;">
            <span>* Equal Contribution</span>
        </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- arXiv Link. -->
              <span class="link-block">
                <a href="https://www.arxiv.org/abs/2412.10493" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Visualignment/SafetyDPO" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX" style="margin-top: -50px; margin-bottom: -50px;">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{liu2024safetydpo,
        title={SafetyDPO: Scalable Safety Alignment for Text-to-Image Generation},
        author={Liu, Runtao and Chieh, Chen I and Gu, Jindong and Zhang, Jipeng and Pi, Renjie and Chen, Qifeng and Torr, Philip and Khakzar, Ashkan and Pizzati, Fabio},
        journal={arXiv preprint arXiv:2412.10493},
        year={2024}
      }
    </code></pre>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation & Background</h2>
        <div class="content has-text-justified">
          <p align="center">
            <img width="500" alt="image" src="https://github.com/user-attachments/assets/61dcc739-958b-4e80-bf14-e33979dada79" />
          </p>
          <p>
            <strong>Safety alignment for T2I.</strong> T2I models released without safety alignment risk to be misused (top). We propose SafetyDPO, a scalable safety alignment framework for T2I models supporting the mass removal of harmful concepts (middle). We allow for scalability by training safety experts focusing on separate categories such as “Hate”, “Sexual”, “Violence”, etc. We then merge the experts with a novel strategy. By doing so, we obtain safety-aligned models, mitigating unsafe content generation (bottom).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-image (T2I) models have become widespread, but their limited safety guardrails expose end users to harmful content and potentially allow for model misuse. Current safety measures are typically limited to text-based filtering or concept removal strategies, able to remove just a few concepts from the model's generative capabilities. In this work, we introduce SafetyDPO, a method for safety alignment of T2I models through Direct Preference Optimization (DPO). We enable the application of DPO for safety purposes in T2I models by synthetically generating a dataset of harmful and safe image-text pairs, which we call CoProV2. Using a custom DPO strategy and this dataset, we train safety experts, in the form of low-rank adaptation (LoRA) matrices, able to guide the generation process away from specific safety-related concepts. Then, we merge the experts into a single LoRA using a novel merging strategy for optimal scaling performance. This expert-based approach enables scalability, allowing us to remove 7 times more harmful concepts from T2I models compared to baselines. SafetyDPO consistently outperforms the state-of-the-art on many benchmarks and establishes new practices for safety alignment in T2I networks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Dataset Generation</h3>
          <p align="center">
            <img width="400" alt="image" src="https://github.com/user-attachments/assets/6f5613e0-dcbf-4831-bd7e-e27c6e81bacb" />
          </p>
          <p>
            For each unsafe concept in different categories, we generate corresponding prompts using an LLM. We generate paired safe prompts using an LLM, minimizing semantic differences. Then, we use the T2I model we intend to align to generate corresponding images for both prompts.
          </p>

          <h3 class="title is-4">Architecture - Improving scaling with safety experts</h3>
          <p align="center">
            <img width="1000" alt="image" src="https://github.com/user-attachments/assets/0c318d46-8e72-4eed-b98b-ab4b163929f1" />
          </p>
          <p>
            <strong>Expert Training and Merging.</strong> First, we use the previously generated prompts and images to train LoRA experts on specific safety categories (left), exploiting our DPO-based losses. Then, we merge all the safety experts with Co-Merge (right). This allows us to achieve general safety experts that produce safe outputs for a generic unsafe input prompt in any category.
          </p>

          <h3 class="title is-4">Experts Merging</h3>
          <p align="center">
            <img width="500" alt="image" src="https://github.com/user-attachments/assets/6e964e7b-e253-4bd0-9fce-0cd0df1793aa" />
          </p>
          <p>
            <strong>Merging Experts with Co-Merge.</strong> (Left) Assuming LoRA experts with the same architecture, we analyze which expert has the highest activation for each weight across all inputs. (Right) Then, we obtain the merged weights from multiple experts by merging only the most active weights per expert.
          </p>

          <p align="center">
            <img width="600" alt="image" src="https://github.com/user-attachments/assets/abb71f4e-a6cd-47dd-a0c3-4906d79e6d34" />
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p align="center">
            <img width="663" alt="image" src="https://github.com/user-attachments/assets/3fecb8e0-80ff-4967-8075-6d0bbc8fcab3" />
          </p>
          <p>
            <strong>Datasets Comparison.</strong> Our LLM-generated dataset, CoProV2, achieves comparable Inappropriate Probability (IP) to human-crafted datasets (UD [44], I2P [51]) and offers a similar scale to CoPro [33]. COCO [32], exhibiting a low IP, is used as a benchmark for image generation with safe prompts as input.
          </p>

          <p align="center">
            <img width="663" alt="image" src="https://github.com/user-attachments/assets/c0471c5a-2ff1-477a-99c5-eb8c69ffb6fa" />
          </p>
          <p>
            <strong>Benchmark.</strong> SafetyDPO achieves the best performance both in generated image alignment (IP) and image quality (FID, CLIPScore) with two T2I models and against 3 methods for SD v1.5. Note that we use CoProV2 only for training; hence, I2P and UD are out-of-distribution. Yet, SafetyDPO allows a robust safety alignment.
            <br><em>Best results are <strong>bold</strong>, and second-best results are <em>underlined</em>.</em>
          </p>

          <p align="center">
            <img width="1000" alt="image" src="https://github.com/user-attachments/assets/8117fe06-4562-4c19-a610-2841a729bd29" />
          </p>
          <p>
            <strong>Qualitative Comparison.</strong> Compared to non-aligned baseline models, SafetyDPO allows the synthesis of safe images for unsafe input prompts. Please note the layout similarity between the unsafe and safe outputs: thanks to our training, only the harmful image traits are removed from the generated images. Concepts are shown in ⟨brackets⟩. Prompts are shortened; for full ones, see the supplementary material.
          </p>

          <p align="center">
            <img width="500" alt="image" src="https://github.com/user-attachments/assets/71b7e03d-1b82-4a1a-9974-c17cf7e2dd17" />
          </p>
          <p>
            <strong>Effectiveness of Merging.</strong> While training a single safety expert across all data (All-single), IP performance is lower or comparable to single experts (previous rows). Instead, by merging safety experts (All-ours), we considerably improve results.
          </p>

          <p align="center">
            <img width="663" alt="image" src="https://github.com/user-attachments/assets/01dd0ccd-c3a7-4868-ab67-975ab7288d48" />
          </p>
          <p>
            <strong>Resistance to Adversarial Attacks.</strong> We evaluate the performance of SafetyDPO and the best baseline, ESD-u, in terms of IP using 4 adversarial attack methods. For a wide range of attacks, we are able to outperform the baselines, advocating for the effectiveness of our scalable concept removal strategy.
          </p>

          <p align="center">
            <img width="663" alt="image" src="https://github.com/user-attachments/assets/54fdb6fd-67dd-457f-ad28-1500fcec8458" />
          </p>
          <p>
            <strong>Ablation Studies.</strong> We check the effects of alternative strategies for DPO, proving that our approach is the best (a). Co-Merge is also the best merging strategy compared to baselines (b). Finally, we verify that scaling data improves our performance (c).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


</body>
</html>
